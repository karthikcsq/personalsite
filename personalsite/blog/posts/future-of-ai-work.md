---
title: "On the Future of Work with AI"
date: "2025-09-16"
summary: "How AI will shape the future of work, and how humanity will actually achieve bigger and better."
---

## Why Write This?

There are a number of experts who have shared their views on the future of work and AI with the world. Yet, I have not been fully convinced that anything I heard will shape up to be true. Obviously, there's no way to be sure, but something tells me that we will not be getting Artificial Superintelligence (ASI) or even AGI within the next 10 years (A bold claim within the AI community, for sure). Let me define what my perception of what AGI should be able to do, and explain why what others may consider AGI will not have the impacts claimed to exist.

Informally, AGI is the idea of a system that achieves equal knowledge and performance on any task to the best humans on the planet at that task. Many proponents of this idea will then go on to claim that, if AGI can be created, an ASI, or a system that achieves *better* knowledge and performance on any task than the best humans on the planet, will come shortly thereafter. Yet, even this claim is weak in the sense that it does not fully capture what humans truly excel at. While humans have been known to pursue knowledge and become skilled at specific cognitive tasks, the true journey that we as a civilization have been on, is vastly different than that of pursuing knowledge and improving at tasks. 

### The Betterment of Civilization

When humans first moved from being hunter-gatherers to a primarily agrarian society, there was a distinct change in the type of cognitive task that was performed. Humans went from being good at the repetitive cognitive task of hunting or gathering, to becoming more focused on the cognitive tasks required of growing food. The industrial revolution transfered this cognitive task to repetitively operating machinery, gaining domain expertise in that area correspondingly. Today, these tasks are distributed across thousands of different repetitive cognitive tasks. This is why people are worried about AI: it's really good at repetitive cognitive tasks. These are tasks that are not fully automatable with discrete operations, but require some information processing that may affect what occurs. In essence however, we are using our brains as an information processing mechanism that maps some incoming information to a task that we have gained expertise in. When this information is clearly definable (an essay prompt, a coding task, data analysis, etc.), AI is very efficient at executing on a task from this information. This is why prompt engineering exists: AI works better when everything is clearly defined (and so do humans). This is also why plumbers and electricians are less scared of AI coming for their jobs - not simply because it's physical labor (although that is part of the reason), but rather, the input information space is messy. There's no clearly defined "language" that can be produced from chaotic physical systems yet. Hence, the models that we describe as AI, which are really Large **Language** Models, are vastly more ineffective in these scenarios. The success of AlphaFold can be attributed in large part to the development of a "language" for proteins, allowing it to iterate and test on a specifically defined set of information.

Yet, it is only recently in the history of humanity that we have developed clearly defined instruction sets for so many of the repetitive cognitive tasks that we perform today. This is the entire goal of the scientific method: define variable and collect measurable observations as a result. However, any scientist will tell you that it is not conducting the actual experiment that is difficult, but rather designing the experiment. Experiments that deal with real world materials, interactions, and complex systems are hard to design, because the wealth of decision that have to be made is simply incomprehensible to define clearly. But something about our minds and our experiences allow us to have grand visions of experiments and products that are not clearly defined, but still make use of these chaotic systems effectively

### So... why?

This is where my argument will weaken considerably, but I will attempt to make it nonetheless. I don't believe that the current models of AI will be able to make use of these chaotic real-world systems and design experiments from vague ideas. It will be able to plan and iterate and execute on experiments *easily*. The next Thomas Edison will no longer have to think of materials to build a lightbulb filament from, but rather, define the experiment that a lightbulb with a light-producing filament needs to exist to the AI clearly - and once this is done, the AI will iterate on elements, compounds, run experiments, and observe real data. It *feels* like AI is doing all our work for us, but really, this is grunt work. It is not the true intellectual pursuit or betterment of society that requires time spent by humans.

## The Future of Work

